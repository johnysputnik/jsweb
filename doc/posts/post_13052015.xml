<?xml version="1.0"?>
<blog>

    <post>
        <title>multivariate linear regression with Scilab</title>
        <date>2015-05-13 09:57:46</date>
        <tag1>AI</tag1>
        <tag2>maths</tag2>
        <tag3>machine learning</tag3>
        <tag4>scilab</tag4>
        <summary>
            An implementation of multivariate linear regression, including normalization,
            using Scilab.
        </summary>

        <contents><![CDATA[
        <p>
    This post outlines some code written to implement a simple multivariate linear
    regression algorithm in <a href="http://www.scilab.org"  target="_blank">Scilab</a>. It was written whilst I was
    completing a course on Machine Learning to be found on Coursera
    <a href="https://www.coursera.org/learn/machine-learning" target="_blank">here</a>.
    The course uses octave for most of it's examples but I chose to write it in
    Scilab, for reasons not really worth going into here.
</p>
<p>
    A description of the algorithm can be found
    <a href="http://www.jsolutions.co.uk/article?title=linear+regression+with+multiple+variables"  target="_blank">here</a>.
</p>
<p>
    First we must initialise the data from an input training set. The X matrix contains
    the variables and the y vector contains the results.  The training set contains
    house prices based on house size and number of bedrooms.
</p>
<pre class="prettyprint lang-matlab">// Initialise data

data = csvRead('ex1data2.txt');
X = data(:, 1:2);
y = data(:, 3);
m = length(y)
</pre>

<p>
    Next the data is normalized according to a standard distribution. This enables the
    algorithm to find convergence more efficiently.
</p>

<pre class="prettyprint lang-matlab">// normalize the data

mu = mean(X, "r");
sigma = stdev(X, "r");

for i = 1:size(X, "c")
    X(:,i) = (X(:,i) - mu(i)) ./ sigma(i);
end;
</pre>

<p>
    Now that the training set variable matrix is normalized, we can add the
    <span style="font-style: italic">x<sub>0</sub></span> variables
    of value 1, needed by the algorithm.
</p>

<pre class="prettyprint lang-matlab">// add x_0 to X

X = [ones(m, 1) X];

</pre>

<p>
    We can now set the parameters for the gradient descent calculations, including
    the number of iterations and the linear equation parameters, theta. We will
    also keep a record of the cost (J) at each iteration, for displaying in a
    graph to ensure convergence to a minimum.
</p>

<pre class="prettyprint lang-matlab">// set parameters for iterations and
// learning rate

alpha = 0.01;
num_iters = 400;
theta = zeros(3, 1);
J_history = zeros(num_iters, 1);
</pre>

<p>
    We can now run the gradient descent algorithm to calculate values
    of theta at convergence.
</p>

<pre class="prettyprint lang-matlab">// run the gradientDescent

for iter = 1:num_iters
    hypothesis = X * theta;
    errors = hypothesis - y;
    change = (alpha * (X' * errors)) / m;
    theta = theta - change;

    J_history(iter) = (sum(((X * theta) - y) .^ 2)) / (2 * m);
end;
</pre>

<p>
    Now that we have completed the gradient descent, we can plot
    the cost value and check that it does actually converge.
</p>

<pre class="prettyprint lang-matlab">// plot the convergence graph

plot(1:size(J_history, "r"), J_history, '-g', 'LineWidth', 1);
xtitle("Convergence of Cost Function", "Iterations", "Cost");
</pre>

<img src="/data/img/multi_example_cost.png" alt="converging cost"/>

<p>
    As we can see that the cost converges, we can now use the values of
    theta (and the normalization parameters) to calculate predicted
    house prices.
</p>

<pre class="prettyprint lang-matlab">//  Estimate the price of a 1650 sq-ft, 3 br house

norm_area = (1650 - mu(1)) / sigma(1);
norm_bedrooms = (3 - mu(2)) / sigma(2);
price = theta(1) + (theta(2) * norm_area) + (theta(3) * norm_bedrooms);
</pre>

<p>
    This code should scale nicely to large numbers of variables in the linear
    equation and training data set.
</p>
<p>
    The complete source file can be found <a href="/data/src/lin-reg.sci" target="_blank">here</a>.
</p>

        ]]></contents>

    </post>

</blog>